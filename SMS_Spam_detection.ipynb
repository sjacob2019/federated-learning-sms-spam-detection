{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting Fraudulent SMS Messages with Federated Learning"
      ],
      "metadata": {
        "id": "IKMbkIkde-ZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies"
      ],
      "metadata": {
        "id": "syJZNDqefDlA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X3h5WQpdX-x"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install sklearn\n",
        "!pip install huggingface_hub\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install numpy requests nlpaug\n",
        "!pip install nltk>=3.4.5\n",
        "!pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece\n",
        "!pip install flwr[simulation]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "deApnsskfF_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import evaluate\n",
        "import random\n",
        "import os\n",
        "\n",
        "import pyarrow as pa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import flwr as fl\n",
        "\n",
        "from collections import OrderedDict\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from math import floor\n",
        "from scipy import sparse\n",
        "from huggingface_hub import login\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as nafc\n",
        "from nlpaug.util import Action\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import CondensedNearestNeighbour, RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, DataCollatorForTokenClassification, DefaultDataCollator\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "NuarO7tge88Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bBDIbc91-0rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Dataset"
      ],
      "metadata": {
        "id": "CbG9olVwfJxA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43_gDA4veOr5"
      },
      "outputs": [],
      "source": [
        "# Downloading original public sms_spam dataset from huggingface and splitting.\n",
        "# The split sms_spam dataset is what I uploaded as sms_spam in my private hub\n",
        "\n",
        "# dataset = load_dataset(\"sms_spam\")\n",
        "# dataset = dataset[\"train\"]\n",
        "# dataset = dataset.train_test_split(test_size=0.2, stratify_by_column=\"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_WBpjEddKcl"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4XFJARsePBL"
      },
      "outputs": [],
      "source": [
        "access_token = \"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\"\n",
        "dataset = load_dataset(\"jyoung2247/sms_spam\", use_auth_token=access_token)\n",
        "# dataset = load_dataset(\"jyoung2247/sms_spam_augmented_synonyms\", use_auth_token=access_token)\n",
        "# #dataset = load_dataset(\"jyoung2247/sms_spam_augmented_context_insert\", use_auth_token=access_token)\n",
        "# dataset = load_dataset(\"jyoung2247/sms_spam_augmented_context_substitute\", use_auth_token=access_token)\n",
        "# #dataset = load_dataset(\"jyoung2247/sms_spam_undersampled_CNN\", use_auth_token=access_token)\n",
        "# dataset = load_dataset(\"jyoung2247/sms_spam_undersampled_random\", use_auth_token=access_token)\n",
        "# #dataset.push_to_hub(\"jyoung2247/sms_spam\", private=True)\n",
        "# dataset = load_dataset(\"jyoung2247/sms_spam_oversampled_SMOTE\", use_auth_token=access_token)\n",
        "# dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balancing Dataset"
      ],
      "metadata": {
        "id": "TMJSv_LoDbyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Random Oversampling"
      ],
      "metadata": {
        "id": "oCVAMp-9DaGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Create numpy arrays from dataset sms messages as x and labels as y\n",
        "x_train = dataset['train']['sms']\n",
        "y_train = dataset['train']['label']\n",
        "x_train = np.array(x_train)\n",
        "x_train = x_train[:, np.newaxis]\n",
        "\n",
        "#Create random over sampler from imbalanced-learn\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "x_train_resampled, y_train_resampled = ros.fit_resample(x_train, y_train)\n",
        "x_train_resampled = x_train_resampled.ravel()\n",
        "\n",
        "#Create a dictionary of resampled x and y\n",
        "dataset_train = {'sms': x_train_resampled, 'label': y_train_resampled}\n",
        "\n",
        "#Create a huggingface dataset from the dictionary\n",
        "dataset_train = Dataset.from_dict(dataset_train)\n",
        "\n",
        "#Encode the dataset_train label column features as ham and spam\n",
        "dataset_train = dataset_train.class_encode_column('label')\n",
        "\n",
        "dataset_train.features['label'].names = ['ham', 'spam']\n",
        "\n",
        "#Create a DatasetDict from the resampled_dataset, which can be pushed to the huggingface hub\n",
        "resampled_dataset = DatasetDict()\n",
        "resampled_dataset['test'] = dataset['test']\n",
        "resampled_dataset['train'] = dataset_train"
      ],
      "metadata": {
        "id": "eRQXiBNiC1pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Random Undersampling"
      ],
      "metadata": {
        "id": "VypYq7FfDYF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create numpy arrays from dataset sms messages as x and labels as y\n",
        "x_train = dataset['train']['sms']\n",
        "y_train = dataset['train']['label']\n",
        "x_train = np.array(x_train)\n",
        "x_train = x_train[:, np.newaxis]\n",
        "\n",
        "\n",
        "#Create random over sampler from imbalanced-learn\n",
        "\n",
        "ros = RandomUnderSampler(random_state=0)\n",
        "x_train_resampled, y_train_resampled = ros.fit_resample(x_train, y_train)\n",
        "x_train_resampled = x_train_resampled.ravel()\n",
        "\n",
        "#Create a dictionary of resampled x and y\n",
        "dataset_train = {'sms': x_train_resampled, 'label': y_train_resampled}\n",
        "dataset_train = Dataset.from_dict(dataset_train)\n",
        "\n",
        "#Create a huggingface dataset from the dictionary\n",
        "dataset_train = dataset_train.class_encode_column('label')\n",
        "\n",
        "#Encode the dataset_train label column features as ham and spam\n",
        "dataset_train.features['label'].names = ['ham', 'spam']\n",
        "\n",
        "#Create a DatasetDict from the resampled_dataset, which can be pushed to the huggingface hub\n",
        "resampled_dataset = DatasetDict()\n",
        "resampled_dataset['test'] = dataset['test']\n",
        "resampled_dataset['train'] = dataset_train"
      ],
      "metadata": {
        "id": "SgEYzTTYC9O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Undersampled CNN"
      ],
      "metadata": {
        "id": "7CnC2EYBDWRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sms\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_train = dataset['train'].map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "\n",
        "x_train = tokenized_train['input_ids']\n",
        "y_train = tokenized_train['label']\n",
        "x_train = np.array(x_train)\n",
        "\n",
        "ros = CondensedNearestNeighbour(n_neighbors=1)\n",
        "x_train_resampled, y_train_resampled = ros.fit_resample(x_train, y_train)\n",
        "x_train_resampled = tokenizer.batch_decode(x_train_resampled, skip_special_tokens=True)\n",
        "\n",
        "dataset_train = {'sms': x_train_resampled, 'label': y_train_resampled}\n",
        "dataset_train = Dataset.from_dict(dataset_train)\n",
        "\n",
        "print(dataset_train)\n",
        "\n",
        "dataset_train = dataset_train.class_encode_column('label')\n",
        "\n",
        "dataset_train.features['label'].names = ['ham', 'spam']\n",
        "\n",
        "resampled_dataset = DatasetDict()\n",
        "resampled_dataset['test'] = dataset['test']\n",
        "resampled_dataset['train'] = dataset_train"
      ],
      "metadata": {
        "id": "sSe3V39NDBMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resampled_dataset.push_to_hub(\"jyoung2247/sms_spam_undersampled_CNN\", private=True)"
      ],
      "metadata": {
        "id": "Rn1mcU2eDEkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented Synonyms"
      ],
      "metadata": {
        "id": "1hluJiqLDTSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitute word by WordNet's synonym\n",
        "\n",
        "augmented_text = train_spam_sms.copy()\n",
        "\n",
        "augmented_train_sms = []\n",
        "\n",
        "#Run 6 iterations of augmenting synonyms to balance the dataset\n",
        "for i in range(6):\n",
        "  aug = naw.SynonymAug(aug_src='wordnet')\n",
        "  augmented_text = aug.augment(augmented_text)\n",
        "  print(augmented_text)\n",
        "  augmented_train_sms = augmented_train_sms + augmented_text\n",
        "  \n",
        "#Remove the extra 325 generated spam entires to ensure there is an equal amount of spam and non-spam\n",
        "augmented_train_sms = augmented_train_sms[:-325]\n",
        "print(\"Length of augmented spam: \", len(augmented_train_sms))"
      ],
      "metadata": {
        "id": "3V8Qf9QADGup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print total spam amount and total ham amount (non-spam)\n",
        "total_spam_count = len(augmented_train_sms) + len(train_spam_sms)\n",
        "total_ham_count = len(dataset['train']['sms']) - len(train_spam_sms)\n",
        "total_sms_count = total_spam_count + total_ham_count\n",
        "print(\"Total spam: \", total_spam_count)\n",
        "print(\"Total ham: \", total_ham_count)\n",
        "print(\"Total sms: \", total_sms_count)"
      ],
      "metadata": {
        "id": "51aOjZBiDIHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set x_train equal to the sms messages in the dataset train split\n",
        "x_train = dataset['train']['sms']\n",
        "# Set y_train equal to the labels in the dataset train split\n",
        "y_train = dataset['train']['label']\n",
        "\n",
        "# Set augmented x_train equal to x_train + the generated spam\n",
        "augmented_x_train = x_train + augmented_train_sms\n",
        "\n",
        "# Set augmented y_train equal to y_train plus a list of 1's of equal size to the generated spam\n",
        "augmented_y_train = y_train + [1] * (len(augmented_x_train) - len(y_train))\n",
        "\n",
        "#Combine the augmented x_train and y_train to shuffle together\n",
        "combined_arr = np.array(augmented_x_train)\n",
        "combined_arr = combined_arr[:, np.newaxis]\n",
        "y_arr = np.array(augmented_y_train)\n",
        "y_arr = y_arr[:, np.newaxis]\n",
        "\n",
        "combined_arr = np.append(combined_arr, y_arr, axis=1)\n",
        "\n",
        "np.random.shuffle(combined_arr)\n",
        "\n",
        "#Separate the augmented x_train and y_train again after shuffling\n",
        "augmented_x_train = combined_arr[:, 0]\n",
        "augmented_y_train = combined_arr[:, 1]\n",
        "\n",
        "#Create a dictionary of augmented x and y\n",
        "dataset_train = {'sms': augmented_x_train, 'label': augmented_y_train}\n",
        "\n",
        "#Create a huggingface dataset from the dictionary\n",
        "dataset_train = Dataset.from_dict(dataset_train)\n",
        "\n",
        "#Encode the dataset_train label column features as ham and spam\n",
        "dataset_train = dataset_train.class_encode_column('label')\n",
        "\n",
        "dataset_train.features['label'].names = ['ham', 'spam']\n",
        "\n",
        "#Create a DatasetDict from the resampled_dataset, which can be pushed to the huggingface hub\n",
        "augmented_dataset = DatasetDict()\n",
        "augmented_dataset['test'] = dataset['test']\n",
        "augmented_dataset['train'] = dataset_train\n"
      ],
      "metadata": {
        "id": "7EeQDUW1DKEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented Context Substitute"
      ],
      "metadata": {
        "id": "WooghEx1DOvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitute word by contextual word embeddings (BERT, DistilBERT, RoBERTA or XLNet)\n",
        "\n",
        "augmented_text = train_spam_sms.copy()\n",
        "\n",
        "augmented_train_sms = []\n",
        "\n",
        "#Run 6 iterations of augmenting synonyms to balance the dataset\n",
        "for i in range(6):\n",
        "  aug = naw.ContextualWordEmbsAug(\n",
        "      model_path='bert-base-uncased', action=\"substitute\", device=\"cuda\")\n",
        "  augmented_text = aug.augment(augmented_text)\n",
        "  print(augmented_text)\n",
        "  augmented_train_sms = augmented_train_sms + augmented_text\n",
        "\n",
        "#Remove the extra 325 generated spam entires to ensure there is an equal amount of spam and non-spam\n",
        "print(\"Length of augmented spam: \", len(augmented_train_sms))"
      ],
      "metadata": {
        "id": "FxhdUlzWDjfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print total spam amount and total ham amount (non-spam)\n",
        "total_spam_count = len(augmented_train_sms) + len(train_spam_sms)\n",
        "total_ham_count = len(dataset['train']['sms']) - len(train_spam_sms)\n",
        "total_sms_count = total_spam_count + total_ham_count\n",
        "print(\"Total spam: \", total_spam_count)\n",
        "print(\"Total ham: \", total_ham_count)\n",
        "print(\"Total sms: \", total_sms_count)"
      ],
      "metadata": {
        "id": "p2NFqoyADk7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set x_train equal to the sms messages in the dataset train split\n",
        "x_train = dataset['train']['sms']\n",
        "# Set y_train equal to the labels in the dataset train split\n",
        "y_train = dataset['train']['label']\n",
        "\n",
        "# Set augmented x_train equal to x_train + the generated spam\n",
        "augmented_x_train = x_train + augmented_train_sms\n",
        "\n",
        "# Set augmented y_train equal to y_train plus a list of 1's of equal size to the generated spam\n",
        "augmented_y_train = y_train + [1] * (len(augmented_x_train) - len(y_train))\n",
        "\n",
        "#Combine the augmented x_train and y_train to shuffle together\n",
        "combined_arr = np.array(augmented_x_train)\n",
        "combined_arr = combined_arr[:, np.newaxis]\n",
        "y_arr = np.array(augmented_y_train)\n",
        "y_arr = y_arr[:, np.newaxis]\n",
        "\n",
        "combined_arr = np.append(combined_arr, y_arr, axis=1)\n",
        "\n",
        "np.random.shuffle(combined_arr)\n",
        "\n",
        "#Separate the augmented x_train and y_train again after shuffling\n",
        "augmented_x_train = combined_arr[:, 0]\n",
        "augmented_y_train = combined_arr[:, 1]\n",
        "\n",
        "#Create a dictionary of augmented x and y\n",
        "dataset_train = {'sms': augmented_x_train, 'label': augmented_y_train}\n",
        "\n",
        "#Create a huggingface dataset from the dictionary\n",
        "dataset_train = Dataset.from_dict(dataset_train)\n",
        "\n",
        "#Encode the dataset_train label column features as ham and spam\n",
        "dataset_train = dataset_train.class_encode_column('label')\n",
        "\n",
        "dataset_train.features['label'].names = ['ham', 'spam']\n",
        "\n",
        "#Create a DatasetDict from the resampled_dataset, which can be pushed to the huggingface hub\n",
        "augmented_dataset = DatasetDict()\n",
        "augmented_dataset['test'] = dataset['test']\n",
        "augmented_dataset['train'] = dataset_train"
      ],
      "metadata": {
        "id": "DCwX4HG2DmRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmented Context Insert"
      ],
      "metadata": {
        "id": "IREaYEBxDrkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert word by contextual word embeddings (BERT, DistilBERT, RoBERTA or XLNet)\n",
        "\n",
        "augmented_text = train_spam_sms.copy()\n",
        "\n",
        "augmented_train_sms = []\n",
        "\n",
        "#Run 6 iterations of augmenting synonyms to balance the dataset\n",
        "for i in range(6):\n",
        "  aug = naw.ContextualWordEmbsAug(\n",
        "      model_path='bert-base-uncased', action=\"insert\", device=\"cuda\")\n",
        "  augmented_text = aug.augment(augmented_text)\n",
        "  print(augmented_text)\n",
        "  augmented_train_sms = augmented_train_sms + augmented_text\n",
        "\n",
        "#Remove the extra 325 generated spam entires to ensure there is an equal amount of spam and non-spam\n",
        "print(\"Length of augmented spam: \", len(augmented_train_sms))"
      ],
      "metadata": {
        "id": "02p8P_aTDu6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print total spam amount and total ham amount (non-spam)\n",
        "total_spam_count = len(augmented_train_sms) + len(train_spam_sms)\n",
        "total_ham_count = len(dataset['train']['sms']) - len(train_spam_sms)\n",
        "total_sms_count = total_spam_count + total_ham_count\n",
        "print(\"Total spam: \", total_spam_count)\n",
        "print(\"Total ham: \", total_ham_count)\n",
        "print(\"Total sms: \", total_sms_count)"
      ],
      "metadata": {
        "id": "r0vQvE49Dwcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set x_train equal to the sms messages in the dataset train split\n",
        "x_train = dataset['train']['sms']\n",
        "# Set y_train equal to the labels in the dataset train split\n",
        "y_train = dataset['train']['label']\n",
        "\n",
        "# Set augmented x_train equal to x_train + the generated spam\n",
        "augmented_x_train = x_train + augmented_train_sms\n",
        "\n",
        "# Set augmented y_train equal to y_train plus a list of 1's of equal size to the generated spam\n",
        "augmented_y_train = y_train + [1] * (len(augmented_x_train) - len(y_train))\n",
        "\n",
        "#Combine the augmented x_train and y_train to shuffle together\n",
        "combined_arr = np.array(augmented_x_train)\n",
        "combined_arr = combined_arr[:, np.newaxis]\n",
        "y_arr = np.array(augmented_y_train)\n",
        "y_arr = y_arr[:, np.newaxis]\n",
        "\n",
        "combined_arr = np.append(combined_arr, y_arr, axis=1)\n",
        "\n",
        "np.random.shuffle(combined_arr)\n",
        "\n",
        "#Separate the augmented x_train and y_train again after shuffling\n",
        "augmented_x_train = combined_arr[:, 0]\n",
        "augmented_y_train = combined_arr[:, 1]\n",
        "\n",
        "#Create a dictionary of augmented x and y\n",
        "dataset_train = {'sms': augmented_x_train, 'label': augmented_y_train}\n",
        "\n",
        "#Create a huggingface dataset from the dictionary\n",
        "dataset_train = Dataset.from_dict(dataset_train)\n",
        "\n",
        "#Encode the dataset_train label column features as ham and spam\n",
        "dataset_train = dataset_train.class_encode_column('label')\n",
        "\n",
        "dataset_train.features['label'].names = ['ham', 'spam']\n",
        "\n",
        "#Create a DatasetDict from the resampled_dataset, which can be pushed to the huggingface hub\n",
        "augmented_dataset = DatasetDict()\n",
        "augmented_dataset['test'] = dataset['test']\n",
        "augmented_dataset['train'] = dataset_train"
      ],
      "metadata": {
        "id": "R-3JhH8TDxza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Oversampled SMOTE"
      ],
      "metadata": {
        "id": "zMmRylgVKGuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The textTransformer pipeline allows for a quick vectorization and normalization of\n",
        "# text to be done\n",
        "\n",
        "textTransformer = Pipeline([\n",
        "  ('vect', CountVectorizer(binary=True)),\n",
        "  ('tfidf', TfidfTransformer(use_idf=True))\n",
        "])\n",
        "\n",
        "# Sets up the SMOTE oversampling hyperparamete and SVC model\n",
        "\n",
        "sm = SMOTE(sampling_strategy='minority', k_neighbors = 5, random_state = 0)\n",
        "\n",
        "clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0, warm_start=True)\n",
        "\n",
        "# Transforms the String sms column of a pandas dataframe into a normalized, vectorized\n",
        "# dense matrix representation\n",
        "\n",
        "def transform(dataframe, fit=False):\n",
        "  if fit:\n",
        "    transformed = textTransformer.fit_transform(dataframe['sms'], dataframe['label'])\n",
        "  else:\n",
        "    transformed = textTransformer.transform(dataframe['sms'])\n",
        "  toRet = pd.DataFrame(data=sparse.csr_matrix.todense(transformed))\n",
        "  toRet.insert(0, 'label', dataframe['label'])\n",
        "  return toRet\n",
        "\n",
        "# Takes in a dataframe containing a dense matrix of vectorized SMS and their labels\n",
        "# and oversamples the SPAM data using SMOTE\n",
        "\n",
        "def oversample(dataframe):\n",
        "  sparseDF = sparse.csr_matrix(dataframe.loc[:, dataframe.columns != \"label\"])\n",
        "  X_res, y_res = sm.fit_resample(sparseDF, dataframe['label'])\n",
        "  toSave = pd.DataFrame(data=sparse.csr_matrix.todense(X_res))\n",
        "  toSave.insert(0, 'label', y_res)\n",
        "  return toSave\n",
        "\n",
        "# Takes in a dataframe containing a dense matrix of vectorized SMS and their labels\n",
        "# and fits the SVC model onto the SMS and labels\n",
        "\n",
        "def fitModel(model, dataframe):\n",
        "  model.fit(sparse.csr_matrix(dataframe.loc[:, dataframe.columns != \"label\"]), dataframe['label'])\n",
        "\n",
        "def fitModelFed(model, dataframe):\n",
        "  model.fit(sparse.csr_matrix(dataframe.loc[:, dataframe.columns != \"label\"]), dataframe['label'])\n",
        "\n",
        "# Takes in a dataframe and returns non-label data\n",
        "\n",
        "def splitLabel(dataframe):\n",
        "  return dataframe.loc[:, dataframe.columns != \"label\"]"
      ],
      "metadata": {
        "id": "1edQTe0OKyHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_dataset allows us to load our pre-split dataset from huggingface\n",
        "\n",
        "access_token = \"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\"\n",
        "orig_ds = load_dataset(\"jyoung2247/sms_spam\", use_auth_token=access_token)\n",
        "\n",
        "# Sets up Jacob's mini-personal dataset\n",
        "\n",
        "test_data2 = [\"PayPal: 939618 is your security code. Don't share your code.\", \"Hello friend I am a prince\", \"Mom is calling you down to dinner\", \"My friend make you rich!\", \"hi\", \"asdasdxcadsadx\"]\n",
        "test_labels = [0, 1, 0, 1, 0, 1]\n",
        "\n",
        "jacobDF = pd.DataFrame({'sms': test_data2, 'label': test_labels})\n",
        "\n",
        "SMS_spam = [\"FRM:www.GiftComps.store (Voucher#UFQKPX625) MSG:CVS Give-Away! $100 After 5min Survey.  (Voucher#UFQKPX625)\",\n",
        "            \"Fr⁬⁮om: ͏⁫Am⁬a⁯z⁯on Ass⁫ista‍nce Message ID: 9086882 M⁭s‍g: We ha⁯ve dete‍cted a prob⁭lem wi⁬th y‌our acco⁪unt inform⁯ation. Ple⁮a͏se ve⁬ri⁭fy y⁫our info⁫r⁪mation cor⁭re⁭ctly. T͏h⁬is i⁪⁪s sim⁮⁭ple st‍e‍p to rec‌o⁮v⁬er yo⁭⁫ur acc‍⁬ount: 1. Si‌g⁪n in to yo‌u⁯r acc‌⁯ount. 2. Fol⁫͏low th͏e st‍e͏ps to s⁭ee y͏ou⁭r ca⁯‍se. 3. Reso⁬⁫lve t⁯he prob‌͏lem by comple⁯‌ting t‌h‍e in⁪st⁪r⁭uc⁬tion. Up⁪⁬date Infor⁫m‌ation H⁫e͏re : https://iuczbpe.shop/oRQZoUL Yo⁫⁮u ca‌n͏'t ac⁯ce‌ss yo⁪u⁭r acco͏u⁫nt un͏t⁬il t‍his proc⁫⁭ess com⁮p⁬lete.\",\n",
        "            \"FRM:www.OprahGives.store (sms_ID=DYOWAPR453) MSG:Oprah's Biggest Give-Away Ever!  (sms_ID=DYOWAPR453)\",\n",
        "            \"Catch loan quote fundsjoy.us..$ \\\"*^^\\\"\",\n",
        "            \"FRM:www.ClaimRefund.info (sms_ID=IIBAONM948) MSG:Netflix is Reimbursing  $75.  Time Left to Claim (36) hr.  (sms_ID=IIBAONM948)\",\n",
        "            \"FRM:www.Oprahs.diet|       (Voucher#DIHF514) MSG:We're Blasting 0prahs Brand New Essential Sweet Dietary Fruit Candy Give-Away  (Voucher#DIHF514)\",\n",
        "            \"JPMORG-BANKING| Online access was limited to unusal activity. Please verify now at:Https://secu34jpmorg.com/?verify\",\n",
        "            \"Hey Detimmeyon. After review fromCity, you have up to 3175 ready. yp0ydv.com/4c3fb3050d\",\n",
        "            \"Cash Received : A payment is set to deposit in the next hour! Text with our special pin of : 0398\",\n",
        "            \"With Autoinsurancemate check full coverage available at 28/month on switch.autoinsurancemate.com Reply STOP to opt out\",\n",
        "            \"grlbooknow24.me %*&85\",\n",
        "            \"(Alert) Antiviru's Is Not Act!v@ted!: kejh.info/91uRmU\"]\n",
        "\n",
        "SMS_verifications = [\"Account: 743902 is your Samsung account verification code.\", \"Your publix.com verification code is: 4871\", \"196558 is your Yahoo verification code\",\n",
        "                     \"Your Discord verification code is: 894635\", \"Your DoorDash verification code is 589421. Do not share this with anyone. We will never contact you to request this code\", \"PayPal: 539618 is your security code. Don't share your code\"]\n",
        "\n",
        "SMS_spam_true_labels = [1] * 12\n",
        "\n",
        "SMS_verifications_true_labels = [0] * 6\n",
        "\n",
        "jacob_msg_all = SMS_spam + SMS_verifications"
      ],
      "metadata": {
        "id": "0ap0Uju2M1Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since fitted SVC model requires same number of features, we first fit_transform\n",
        "# the textTransformer onto the training data. Then, we transform the test data\n",
        "# for use in predicting\n",
        "\n",
        "train_data = transform(pd.DataFrame.from_dict(orig_ds['train']), fit=True)\n",
        "test_data = transform(pd.DataFrame.from_dict(orig_ds['test']), fit=False)\n",
        "\n",
        "jacob_msg = textTransformer.transform(jacob_msg_all)\n",
        "jacob_msg_labels = SMS_spam_true_labels + SMS_verifications_true_labels\n",
        "\n",
        "# After transforming all the data, we user SMOTE to oversample the SPAM in the training\n",
        "# data before fitting our PassiveAggressiveClassifier onto it\n",
        "\n",
        "oversampled = oversample(train_data)\n",
        "\n",
        "# # Converts the newly transformed and oversampling training and testing data into\n",
        "# # a DatasetDict to save in huggingface\n",
        "\n",
        "# dataToSendTrain = Dataset.from_pandas(oversampled)\n",
        "# dataToSendTest = Dataset.from_pandas(test)\n",
        "\n",
        "# resampled_dataset_SMOTE = DatasetDict({'train': dataToSendTrain, 'test':dataToSendTest})\n",
        "\n",
        "# # Option for CSV\n",
        "\n",
        "# test.to_csv('SMOTE_Testing.csv', index=False)\n",
        "\n",
        "# resampled_dataset_SMOTE.push_to_hub(\"jyoung2247/sms_spam_oversampled_SMOTE\", private=True)"
      ],
      "metadata": {
        "id": "0AiygZP4LMEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Centralized Model"
      ],
      "metadata": {
        "id": "g6_25wXBIPMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Centralized Model setup"
      ],
      "metadata": {
        "id": "gml7n9yIfSEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check count of spam in dataset\n",
        "test_labels = dataset['test']['label']\n",
        "print(\"Total test size\", len(test_labels))\n",
        "test_spam_inds = np.argwhere(test_labels)\n",
        "print(\"Count of spam in test set\", test_spam_inds.shape[0])\n",
        "\n",
        "train_labels = dataset['train']['label']\n",
        "print(\"Total train size\", len(train_labels))\n",
        "train_spam_inds = np.argwhere(train_labels)\n",
        "print(\"Count of spam in train set\", train_spam_inds.shape[0])\n",
        "\n",
        "train_spam_sms = dataset['train']['sms']\n",
        "train_spam_sms = np.array(train_spam_sms)\n",
        "train_spam_sms = train_spam_sms[train_spam_inds]\n",
        "print(train_spam_sms.shape)\n",
        "train_spam_sms = train_spam_sms.ravel()\n",
        "train_spam_sms = train_spam_sms.tolist()"
      ],
      "metadata": {
        "id": "Evd_QnOCDzm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize input for use with bert-tiny\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sms\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "_pgWv4LHD06x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load metrics\n",
        "\n",
        "metric_f1 = evaluate.load(\"f1\")\n",
        "metric_acc = evaluate.load(\"accuracy\")\n",
        "metric_prec = evaluate.load(\"precision\")\n",
        "metric_rec = evaluate.load(\"recall\")"
      ],
      "metadata": {
        "id": "DQRNCbCpD3il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to compute metrics\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"f1\": metric_f1.compute(predictions=predictions, references=labels, average=\"micro\"), \"acc\": metric_acc.compute(predictions=predictions, references=labels), \"prec\": metric_prec.compute(predictions=predictions, references=labels), \"rec\": metric_rec.compute(predictions=predictions, references=labels)}"
      ],
      "metadata": {
        "id": "1bikIGo0D4oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set training arguments. Most are left as default\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=2)"
      ],
      "metadata": {
        "id": "lsv-PihNWrUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train centralized model"
      ],
      "metadata": {
        "id": "Aiwh2L1bF2DV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use GPU if available and initialize pretrained model\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=2).to(device)"
      ],
      "metadata": {
        "id": "nlmqJ_hgF_HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize huggingface trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_datasets['train'],\n",
        "   eval_dataset=tokenized_datasets['test'],\n",
        "   compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "cM3N2nLqD7CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "IKdhDrNVD8LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount google drive to save model or access model weights\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "id": "KnPIiB2ZD9Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model weights\n",
        "trainer.save_model(\"/content/drive/MyDrive/CS6220/model_base2\")"
      ],
      "metadata": {
        "id": "2X3De9ZQD-m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load saved centralized model"
      ],
      "metadata": {
        "id": "QV-Qi-P5GH7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model with saved weights\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/CS6220/model_base\")"
      ],
      "metadata": {
        "id": "t6yCKas7EAHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create trainer object\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "n3YFW_22EBVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate centralized model"
      ],
      "metadata": {
        "id": "IAo14jBPGQ8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run to Evaluate model\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "KCL9GVb8ECXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict a single SMS label with model"
      ],
      "metadata": {
        "id": "ZUj1WNkgajOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SMS_to_predict = \"testSMS\"\n",
        "\n",
        "encoding = tokenizer(SMS_to_predict, padding=\"max_length\", truncation=True, return_tensors='pt', max_length=512)\n",
        "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
        "\n",
        "outputs = trainer.model(**encoding)\n",
        "logits = outputs.logits\n",
        "\n",
        "softmax = torch.nn.Softmax(dim=0)\n",
        "probs = softmax(logits.squeeze())\n",
        "\n",
        "predictionProb, predictionIndex = torch.max(probs, axis=0)\n",
        "\n",
        "print(predictionProb)\n",
        "print(predictionIndex)\n",
        "\n",
        "#get label from prediction\n",
        "labels = dataset[\"train\"].features[\"label\"].names\n",
        "print(labels[predictionIndex.item()])\n",
        "print(predictionProb.item())"
      ],
      "metadata": {
        "id": "XduQLMvEanXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate confusion matrix on dataset test data"
      ],
      "metadata": {
        "id": "5cDJJTqaHQmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select part of the dataset to predict and evaluate accuracy. GPU memory is limited so we can't predict the entire test set at once\n",
        "\n",
        "test_data = dataset[\"test\"]\n",
        "y_test = test_data[\"sms\"][0:1000]\n",
        "y_true = test_data[\"label\"][0:1000]\n",
        "encoding = tokenizer(y_test, padding=\"max_length\", truncation=True, return_tensors='pt', max_length=512)\n",
        "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
        "\n",
        "outputs = trainer.model(**encoding)"
      ],
      "metadata": {
        "id": "u-2C4MP3EDgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = outputs.logits\n",
        "logits.shape"
      ],
      "metadata": {
        "id": "MvNvleqkHJ-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = torch.nn.Softmax(dim=0)\n",
        "probs = softmax(logits.squeeze())\n",
        "predictionProbs, predictionIndicies = torch.max(probs, dim=1)\n",
        "y_pred = predictionIndicies.tolist()\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])"
      ],
      "metadata": {
        "id": "mPXNDe7XHLRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Legitimate', 'Spam']).plot()"
      ],
      "metadata": {
        "id": "035EfMcYH43G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate confusion matrix on real world spam"
      ],
      "metadata": {
        "id": "ZtbWFaisHUmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate model's performance on real world spam and verification text messages\n",
        "\n",
        "SMS_spam = [\"FRM:www.GiftComps.store (Voucher#UFQKPX625) MSG:CVS Give-Away! $100 After 5min Survey.  (Voucher#UFQKPX625)\",\n",
        "            \"Fr⁬⁮om: ͏⁫Am⁬a⁯z⁯on Ass⁫ista‍nce Message ID: 9086882 M⁭s‍g: We ha⁯ve dete‍cted a prob⁭lem wi⁬th y‌our acco⁪unt inform⁯ation. Ple⁮a͏se ve⁬ri⁭fy y⁫our info⁫r⁪mation cor⁭re⁭ctly. T͏h⁬is i⁪⁪s sim⁮⁭ple st‍e‍p to rec‌o⁮v⁬er yo⁭⁫ur acc‍⁬ount: 1. Si‌g⁪n in to yo‌u⁯r acc‌⁯ount. 2. Fol⁫͏low th͏e st‍e͏ps to s⁭ee y͏ou⁭r ca⁯‍se. 3. Reso⁬⁫lve t⁯he prob‌͏lem by comple⁯‌ting t‌h‍e in⁪st⁪r⁭uc⁬tion. Up⁪⁬date Infor⁫m‌ation H⁫e͏re : https://iuczbpe.shop/oRQZoUL Yo⁫⁮u ca‌n͏'t ac⁯ce‌ss yo⁪u⁭r acco͏u⁫nt un͏t⁬il t‍his proc⁫⁭ess com⁮p⁬lete.\",\n",
        "            \"FRM:www.OprahGives.store (sms_ID=DYOWAPR453) MSG:Oprah's Biggest Give-Away Ever!  (sms_ID=DYOWAPR453)\",\n",
        "            \"Catch loan quote fundsjoy.us..$ \\\"*^^\\\"\",\n",
        "            \"FRM:www.ClaimRefund.info (sms_ID=IIBAONM948) MSG:Netflix is Reimbursing  $75.  Time Left to Claim (36) hr.  (sms_ID=IIBAONM948)\",\n",
        "            \"FRM:www.Oprahs.diet|       (Voucher#DIHF514) MSG:We're Blasting 0prahs Brand New Essential Sweet Dietary Fruit Candy Give-Away  (Voucher#DIHF514)\",\n",
        "            \"JPMORG-BANKING| Online access was limited to unusal activity. Please verify now at:Https://secu34jpmorg.com/?verify\",\n",
        "            \"Hey Detimmeyon. After review fromCity, you have up to 3175 ready. yp0ydv.com/4c3fb3050d\",\n",
        "            \"Cash Received : A payment is set to deposit in the next hour! Text with our special pin of : 0398\",\n",
        "            \"With Autoinsurancemate check full coverage available at 28/month on switch.autoinsurancemate.com Reply STOP to opt out\",\n",
        "            \"grlbooknow24.me %*&85\",\n",
        "            \"(Alert) Antiviru's Is Not Act!v@ted!: kejh.info/91uRmU\"]\n",
        "\n",
        "\n",
        "SMS_spam_true_labels = [1] * 12\n",
        "\n",
        "encoding = tokenizer(SMS_spam, padding=\"max_length\", truncation=True, return_tensors='pt', max_length=512)\n",
        "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
        "\n",
        "outputs = trainer.model(**encoding)"
      ],
      "metadata": {
        "id": "UYPhFX0rEEqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = outputs.logits\n",
        "logits.shape\n",
        "y_true = SMS_spam_true_labels"
      ],
      "metadata": {
        "id": "y_suMDgFEG0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = torch.nn.Softmax(dim=0)\n",
        "probs = softmax(logits.squeeze())\n",
        "predictionProbs, predictionIndicies = torch.max(probs, dim=1)\n",
        "y_pred = predictionIndicies.tolist()\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])"
      ],
      "metadata": {
        "id": "Xuh8NbZqEH-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Legitimate', 'Spam']).plot()"
      ],
      "metadata": {
        "id": "hje_6QTTH3-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate confusion matrix on real world verification sms messages"
      ],
      "metadata": {
        "id": "pBuAlAFLHoP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate model's performance on real world verification text messages\n",
        "\n",
        "\n",
        "SMS_verifications = [\"Account: 743902 is your Samsung account verification code.\", \"Your publix.com verification code is: 4871\", \"196558 is your Yahoo verification code\",\n",
        "                     \"Your Discord verification code is: 894635\", \"Your DoorDash verification code is 589421. Do not share this with anyone. We will never contact you to request this code\", \"PayPal: 539618 is your security code. Don't share your code\"]\n",
        "\n",
        "SMS_verifications_true_labels = [0] * 6\n",
        "\n",
        "encoding = tokenizer(SMS_verifications, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
        "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
        "\n",
        "outputs = trainer.model(**encoding)"
      ],
      "metadata": {
        "id": "r9q_q7zAHrva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = outputs.logits\n",
        "logits.shape\n",
        "y_true = SMS_verifications_true_labels"
      ],
      "metadata": {
        "id": "qR_2zQUKIAmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = torch.nn.Softmax()\n",
        "probs = softmax(logits.squeeze())\n",
        "predictionProbs, predictionIndicies = torch.max(probs, dim=1)\n",
        "y_pred = predictionIndicies.tolist()\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])"
      ],
      "metadata": {
        "id": "nkUpD5knICZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Legitimate', 'Spam']).plot()"
      ],
      "metadata": {
        "id": "0-mzTAaLELRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sources: https://huggingface.co/docs/transformers/training\n",
        "#https://discuss.huggingface.co/t/how-to-save-my-model-to-use-it-later/20568\n",
        "#https://colab.research.google.com/drive/1U7SX7jNYsNQG5BY1xEQQHu48Pn6Vgnyt?usp=sharing"
      ],
      "metadata": {
        "id": "gZMFQ6kbH1b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SMOTE Centralized Model"
      ],
      "metadata": {
        "id": "QfJ6IZ88L-4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit PassiveAggressiveClassifier onto Data\n",
        "\n",
        "fitModel(clf, oversampled)"
      ],
      "metadata": {
        "id": "-OPiIraSMB3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate score from testing data\n",
        "\n",
        "clf.score(splitLabel(test), test['label'])\n",
        "\n",
        "# Generate Confusion Matrix from testing data\n",
        "\n",
        "predictions1 = clf.predict(splitLabel(test))\n",
        "cm1 = confusion_matrix(test['label'], predictions1)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm1, display_labels=['Legitimate', 'Spam']).plot()\n",
        "\n",
        "scores = precision_recall_fscore_support(test['label'], predictions1, average='macro')\n",
        "print(clf.score(splitLabel(test), test['label']))\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "C5hx0gUVMD9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform jacob's dataset\n",
        "\n",
        "jacob = transform(pd.DataFrame.from_dict(jacobDF), fit=False)\n",
        "\n",
        "# Generate score from jacob's data\n",
        "\n",
        "clf.score(splitLabel(jacob), jacob['label'])\n",
        "\n",
        "# Generate Confusion Matrix from jacob's data\n",
        "\n",
        "\n",
        "predictions2 = clf.predict(splitLabel(jacob))\n",
        "cm2 = confusion_matrix(jacob['label'], predictions2)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=['Legitimate', 'Spam']).plot()"
      ],
      "metadata": {
        "id": "PeybpFYbMGUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jacobscores = precision_recall_fscore_support(jacob['label'], predictions2, average='macro')\n",
        "# print(clf.score(splitLabel(jacob), jacob['label']))\n",
        "# print(jacobscores)\n",
        "\n",
        "# predictions3 = clf.predict(jacob_msg)\n",
        "# cm3 = confusion_matrix(splitLabel(jacob), predictions2)\n",
        "# ConfusionMatrixDisplay(confusion_matrix=cm3, display_labels=['Legitimate', 'Spam']).plot()"
      ],
      "metadata": {
        "id": "QXGlwpq_MIwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jacobscores2 = precision_recall_fscore_support(jacob_msg_labels, predictions3, average='macro')\n",
        "# print(clf.score(jacob_msg, jacob_msg_labels))\n",
        "# print(jacobscores2)\n",
        "\n",
        "# predictions3 = clf.predict(jacob_msg)\n",
        "# cm3 = confusion_matrix(jacob_msg_labels, predictions3)\n",
        "# ConfusionMatrixDisplay(confusion_matrix=cm3, display_labels=['Legitimate', 'Spam']).plot()\n"
      ],
      "metadata": {
        "id": "nzIluZMGMLob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federated Learning Models"
      ],
      "metadata": {
        "id": "NQrnQ7KJfq_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Partitioning"
      ],
      "metadata": {
        "id": "Y_HBvB-2ftd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_names = [\n",
        "  'sms_spam_augmented_synonyms',\n",
        "  'sms_spam_augmented_context_insert',\n",
        "  'sms_spam_augmented_context_substitute',\n",
        "  'sms_spam_undersampled_CNN',\n",
        "  'sms_spam_undersampled_random',\n",
        "  'sms_spam_naive_oversampled',    \n",
        "  'sms_spam_oversampled_SMOTE',\n",
        "]\n",
        "\n",
        "def get_splits(train_labels, percent_spam):\n",
        "  \n",
        "  train_labels = np.asarray(train_labels).astype(\"int\")\n",
        "  num_spam = len(train_labels[train_labels == 1])\n",
        "  client_size = len(train_labels) // 5\n",
        "  # leftovers = len(train_labels) % 5\n",
        "  spam_counts = []\n",
        "  for i, percent in enumerate(percent_spam):\n",
        "    spam_counts.append(floor(percent * client_size))\n",
        "  legit_counts = spam_counts[::-1]\n",
        "  leftovers = int(len(train_labels) / 2 - sum(spam_counts))\n",
        "  # print(leftovers)\n",
        "  # [0, 1, 2, 3, 4]\n",
        "  # print(legit_counts)\n",
        "  # print(spam_counts)\n",
        "  for i in range(leftovers):\n",
        "    spam_percentage = percent_spam[i % 5]\n",
        "    if spam_percentage == 1:\n",
        "      spam_counts[i % 5] += 1\n",
        "      legit_counts[4 - (i % 5)] += 1\n",
        "    elif spam_percentage == 0:\n",
        "      legit_counts[i % 5] += 1\n",
        "      spam_counts[4 - (i % 5)] += 1\n",
        "    else:\n",
        "      legit_counts[i % 5] += 1\n",
        "      spam_counts[i % 5] += 1\n",
        "  # print(legit_counts)\n",
        "  # print(spam_counts)\n",
        "  # print()\n",
        "  \n",
        "  assert(sum(spam_counts) == sum(legit_counts))\n",
        "  assert(sum(spam_counts) + sum(legit_counts) == len(train_labels)), f'Difference is {len(train_labels) - sum(spam_counts) - sum(legit_counts)}'\n",
        "  return list(zip(legit_counts, spam_counts))\n",
        "\n",
        "\n",
        "def split(train_labels, split_vals):\n",
        "  legit_indices = np.argwhere(1 - train_labels).flatten()\n",
        "  spam_indices = np.argwhere(train_labels).flatten()\n",
        "  indices = []\n",
        "  for legit_count, spam_count in split_vals:\n",
        "    legit = np.random.choice(legit_indices, legit_count, replace=False).flatten()\n",
        "    spam = np.random.choice(spam_indices, spam_count, replace=False).flatten()\n",
        "    combined = np.hstack((legit, spam))\n",
        "    np.random.shuffle(combined)\n",
        "    indices.append(combined)\n",
        "  return indices\n",
        "\n",
        "\n",
        "\n",
        "def generate_splits(trainingData):\n",
        "  # train_data = np.asarray(dataset['train']['sms'])\n",
        "    # train_labels = np.asarray(dataset['train']['label'])\n",
        "    if type(trainingData) != type(pd.DataFrame()):\n",
        "      trainingData = trainingData['train']\n",
        "    train_labels = np.asarray(trainingData['label']) # SMOTE\n",
        "    print(\"Num Spam: \", len(train_labels[train_labels == 1]), \"Num Legitimate: \", len(train_labels[train_labels == 0]))\n",
        "\n",
        "    # train_data = np.asarray(df.drop(['label'], axis=1).to_numpy())\n",
        "    # train_labels = np.asarray(df['label'].to_numpy())\n",
        "    # del df\n",
        "\n",
        "    # letters = string.ascii_lowercase\n",
        "    # train_data = np.array([''.join(random.choice(letters) for i in range(10)) for _ in range(7396)])\n",
        "    # train_labels = np.hstack((np.zeros(3698), np.ones(3698)))\n",
        "    # np.random.shuffle(train_labels)\n",
        "\n",
        "    # train_data_legit = train_data[train_labels == 0]#[:598]\n",
        "    # train_data_spam = train_data[train_labels == 1]\n",
        "    train_labels_legit = train_labels[train_labels == 0]#[:598]\n",
        "    train_labels_spam = train_labels[train_labels == 1]\n",
        "    # print(len(train_labels_spam), len(train_labels_legit))\n",
        "\n",
        "    '''\n",
        "    EDIT THESE THREE ARRAYS BELOW TO CHANGE PROB DISTRIBUTION (ARRAY SPECIFIES SPAM DISTRIBUTION)\n",
        "\n",
        "    THEY MUST AVERAGE OUT TO 0.5 OR THIS WONT WORK,\n",
        "\n",
        "    IF YOU WANT TO MAKE THE SPLITS YOURSELF, DO SOMETHING LIMILAR TO THE COMMENTED OUT LINE BELOW\n",
        "\n",
        "    MAKE A LIST OF TUPLES, ONE TUPLE FOR EACH CLIENT (NUM_LEGITIMATE, NUM_SPAM)\n",
        "    '''\n",
        "    split_1_vals = get_splits(train_labels, [.5, .5, .5, .5, .5]) \n",
        "    #split_1_vals = [(2250, 250), (400, 100), (248, 248), (481, 0), (482, 0)] # Percent Spam: [10, 20, 50, 0, 0]\n",
        "    split_2_vals = get_splits(train_labels, [0, 0, .5, 1, 1]) # [(600, 150), (0, 200), (594, 248), (1333, 0), (1334, 0)] # Percent Spam: [20. 100, 33, 0, 0]\n",
        "    split_3_vals = get_splits(train_labels, [0.15, 0.3, .5, .7, .85])# [(772, 119), (772, 119), (772, 120), (772, 120), (773, 120)] # Percent Spam: [13, 13, 13, 13, 13]\n",
        "    print(split_2_vals)\n",
        "\n",
        "\n",
        "    split_1_indices = split(train_labels, split_1_vals)\n",
        "    split_2_indices = split(train_labels, split_2_vals)\n",
        "    split_3_indices = split(train_labels, split_3_vals)\n",
        "    total_data = [split_1_indices, split_2_indices, split_3_indices]\n",
        "\n",
        "    dataframe_info = {\n",
        "        'indices': [],\n",
        "        'experiment': [],\n",
        "        'client_id': [],\n",
        "    }\n",
        "\n",
        "    for experiment_no in range(3):\n",
        "      for client_id in range(5):\n",
        "        curr_data = total_data[experiment_no][client_id]\n",
        "        experiment = [int(experiment_no)] * len(curr_data)\n",
        "        client_ids = [int(client_id)] * len(curr_data)\n",
        "        dataframe_info['indices'] += curr_data.astype('int').tolist()\n",
        "        dataframe_info['experiment'] += experiment\n",
        "        dataframe_info['client_id'] += client_ids\n",
        "\n",
        "    for arr in dataframe_info.values():\n",
        "      print(len(arr))\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dataframe_info)\n",
        "    # df.head()\n",
        "    # from huggingface_hub import login\n",
        "\n",
        "    # login()\n",
        "\n",
        "    # from datasets import Dataset\n",
        "    # from datasets import DatasetDict\n",
        "\n",
        "    # fed_split = Dataset.from_dict(dataframe_info)\n",
        "\n",
        "    # fed_split.push_to_hub(f\"jyoung2247/split_{dataset_name}\", private=True)\n",
        "\n",
        "    return df\n",
        "  "
      ],
      "metadata": {
        "id": "FuzIYdYJjjUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Federated Learning Models"
      ],
      "metadata": {
        "id": "Dxg45aa2NS3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, trainloader, epochs, device='cpu'):\n",
        "    optimizer = torch.optim.AdamW(net.parameters(), lr=5e-5)\n",
        "    net.train()\n",
        "    for _ in range(epochs):\n",
        "        for batch in trainloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = net(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "def test(net, testloader, device='cpu'):\n",
        "    loss = 0\n",
        "    _metric_f1 = evaluate.load(\"f1\")\n",
        "    _metric_acc = evaluate.load(\"accuracy\")\n",
        "    _metric_prec = evaluate.load(\"precision\")\n",
        "    _metric_rec = evaluate.load(\"recall\")\n",
        "\n",
        "    net.eval()\n",
        "    for batch in testloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = net(**batch)\n",
        "        logits = outputs.logits\n",
        "        loss += outputs.loss.item()\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        \n",
        "        _metric_f1.add_batch(predictions=predictions, references=batch['labels'])\n",
        "        _metric_acc.add_batch(predictions=predictions, references=batch['labels'])\n",
        "        _metric_prec.add_batch(predictions=predictions, references=batch['labels'])\n",
        "        _metric_rec.add_batch(predictions=predictions, references=batch['labels']) \n",
        "\n",
        "    loss /= len(testloader.dataset)\n",
        "\n",
        "    metrics = {'f1': _metric_f1.compute()['f1'], 'acc': _metric_acc.compute()['accuracy'], 'prec': _metric_prec.compute()['precision'], 'rec':_metric_rec.compute()['recall']} \n",
        "    \n",
        "    return loss, metrics\n",
        "\n",
        "raw_datasets = load_dataset(\"jyoung2247/sms_spam\", use_auth_token=access_token)\n",
        "\n",
        "\n",
        "# Change for dataset v\n",
        "mod_dataset = None\n",
        "experiment = -1\n",
        "\n",
        "df = None\n",
        "\n",
        "\n",
        "def load_data(cid, batch_size=32):\n",
        "\n",
        "    if experiment != -1:\n",
        "        cid = int(cid)\n",
        "        training = pd.DataFrame.from_dict(mod_dataset['train'])\n",
        "        data = training.iloc[df.loc[(df['experiment'] == experiment) * (df['client_id'] == cid)]['indices']]\n",
        "        raw_datasets['train'] = Dataset.from_pandas(data)\n",
        "        raw_datasets['train'] = raw_datasets['train'].remove_columns(['__index_level_0__'])\n",
        "      \n",
        "    else:\n",
        "        raw_datasets['train'] = mod_dataset['train']\n",
        "        n = len(raw_datasets['train'])\n",
        "        population_0 = random.sample([i for i in range(n)], n//5)\n",
        "\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
        "    def tokenize_function(examples):\n",
        "        examples['label'] = [int(x) for x in examples['label']]\n",
        "        return tokenizer(examples[\"sms\"], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "    if experiment == -1:\n",
        "        tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].select(population_0)\n",
        "    tokenized_datasets[\"test\"] = tokenized_datasets[\"test\"] \n",
        "    st1 = sum(tokenized_datasets['train']['label'])\n",
        "    print(len(tokenized_datasets['train']) - st1, st1)\n",
        "\n",
        "    data_collator = DefaultDataCollator()\n",
        "    trainloader = DataLoader(\n",
        "        tokenized_datasets[\"train\"],\n",
        "        shuffle=True,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "    testloader = DataLoader(\n",
        "        tokenized_datasets[\"test\"], \n",
        "        batch_size=batch_size, \n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid, checkpoint_name='fed_boi') -> None:\n",
        "        super().__init__()\n",
        "        self.cid = cid\n",
        "        access_token = \"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\"\n",
        "        net = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\",\n",
        "        num_labels=2, use_auth_token=access_token)\n",
        "        self.device='cuda:0' \n",
        "        self.trainloader, self.testloader = load_data(cid, 8)\n",
        "        self.net = net.to(self.device)\n",
        "        self.checkpoint_name = checkpoint_name\n",
        "        \n",
        "    def get_parameters(self, config):\n",
        "        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        params_dict = zip(self.net.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        self.net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.set_parameters(parameters)\n",
        "        print(\"Training Started...\")\n",
        "        train(self.net, self.trainloader, epochs=1, device=self.device)\n",
        "        print(\"Training Finished.\")\n",
        "        self.net.save_pretrained(f\"/content/drive/MyDrive/CS6220/{self.checkpoint_name}\")\n",
        "        return self.get_parameters(config={}), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.set_parameters(parameters)\n",
        "        loss, metrics = test(self.net, self.testloader, device=self.device)\n",
        "        return float(loss), len(self.testloader), metrics\n",
        "\n",
        "\n",
        "  \n",
        "def fedmlExp(checkpoint_name = 'fed_boi'):\n",
        "    pool_size = 5 \n",
        "    \n",
        "    def get_evaluate_fn():\n",
        "        def eval(server_round, parameters, config):\n",
        "            model = FlowerClient(0)\n",
        "            device = torch.device('cuda:0')\n",
        "            model.set_parameters(parameters)\n",
        "            model.net.to(device)  \n",
        "            loss, _, metrics = model.evaluate(parameters, {})\n",
        "            return loss, metrics\n",
        "\n",
        "        return eval\n",
        "    def fit_config(server_round):\n",
        "        return {}\n",
        "    \n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=0.1,\n",
        "        fraction_evaluate=0.1,\n",
        "        min_fit_clients=pool_size,\n",
        "        min_evaluate_clients=1,\n",
        "        min_available_clients=pool_size,  # All clients should be available\n",
        "        on_fit_config_fn=fit_config,\n",
        "        evaluate_fn=get_evaluate_fn(),  # centralised evaluation of global mode\n",
        "    )\n",
        "\n",
        "    def client_fn(cid):\n",
        "        return FlowerClient(cid, checkpoint_name)\n",
        "    \n",
        "    client_resources = {\n",
        "       \"num_cpus\": 1,\n",
        "       \"num_gpus\": 0.2,\n",
        "    } \n",
        "\n",
        "    # start simulation\n",
        "    fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=pool_size,\n",
        "        config=fl.server.ServerConfig(num_rounds=4),\n",
        "        client_resources = client_resources,\n",
        "        strategy=strategy,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "rFMj5byAbkaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOTE\n",
        "\n",
        "\n",
        "os.makedirs('/content/params', exist_ok=True)\n",
        "df = generate_splits(oversampled)\n",
        "\n",
        "def smoteFL():\n",
        "\n",
        "    # access_token = \"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\"\n",
        "    # raw_datasets = load_dataset(\"jyoung2247/sms_spam_oversampled_SMOTE\", use_auth_token=access_token)\n",
        "\n",
        "    def getData(cid):\n",
        "      cid = int(cid)\n",
        "      #Set df['experiment'] to desired split ratio: 0- [.5, .5, .5, .5, .5], 1- [0, 0, .5, 1, 1], 2- [.15, .30, .50, .70, .85]\n",
        "\n",
        "      data = oversampled.iloc[df.loc[(df['experiment'] == experiment) * (df['client_id'] == cid)]['indices']]\n",
        "      X_train = splitLabel(data)\n",
        "      y_train = data['label']\n",
        "      return X_train, y_train\n",
        "\n",
        "    def getDataRandom(cid):\n",
        "      cid = int(cid)\n",
        "      data = splits[cid]\n",
        "      X_train = splitLabel(data)\n",
        "      y_train = data['label']\n",
        "      return X_train, y_train\n",
        "    \n",
        "    X_test = splitLabel(test_data)\n",
        "    y_test = test_data['label']\n",
        "\n",
        "\n",
        "    rf = PassiveAggressiveClassifier(max_iter=1000, random_state=0, warm_start=True)\n",
        "\n",
        "    class SmoteClient(fl.client.NumPyClient):\n",
        "      def __init__(self, cid ) -> None:\n",
        "          super().__init__()\n",
        "          self.cid = cid\n",
        "          \n",
        "          # self.rf = RandomForestClassifier(max_depth=2, random_state=0, warm_start=True)\n",
        "      def get_parameters(self, config):\n",
        "             \n",
        "          params = rf.get_params()\n",
        "          p = []\n",
        "          if hasattr(rf, 'coef_'):\n",
        "            p = [\n",
        "              rf.coef_,\n",
        "              rf.intercept_,\n",
        "              rf.classes_,\n",
        "            ]\n",
        "        \n",
        "        \n",
        "          return p\n",
        "          # for k,v in params.items():\n",
        "          #   if isinstance(v, np.ndarray) or isinstance(v, float) or isinstance(v, int):\n",
        "          #     np.save(f'/content/params/{k}::{self.cid}.npy', v, allow_pickle=True)\n",
        "          #     p.append(f'/content/params/{k}::{self.cid}.npy') \n",
        "          # return p\n",
        "\n",
        "      def set_parameters(self, parameters):\n",
        "          if len(parameters) != 0:\n",
        "            rf.coef_ = parameters[0]\n",
        "            rf.intercept_ = parameters[1]\n",
        "            rf.classes_ = parameters[2]\n",
        "\n",
        "      def fit(self, parameters, config):\n",
        "          if config['rnd'] > 1:\n",
        "            self.set_parameters(parameters)\n",
        "          print(\"Training Started...\")\n",
        "          if experiment > -1:\n",
        "            X_train, y_train = getData(self.cid)\n",
        "          else:\n",
        "            X_train, y_train = getDataRandom(self.cid)\n",
        "          rf.fit(X_train, y_train)\n",
        "          print(\"Training Finished.\")\n",
        "          # self.net.save_pretrained(\"/content/drive/MyDrive/CS6220/fed_boi_1\")\n",
        "          \n",
        "          return self.get_parameters(config), len(X_train), {}\n",
        "\n",
        "      def evaluate(self, parameters, config):\n",
        "        \n",
        "          self.set_parameters(parameters)\n",
        "          accuracy = rf.score(X_test, y_test)\n",
        "          y_pred = rf.predict(X_test)\n",
        "          p, r, f, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
        "          return accuracy, len(X_test), {'acc': accuracy, 'prec': p, 'rec': r, 'f1': f}\n",
        "        \n",
        "    pool_size = 5\n",
        "    \n",
        "    def get_evaluate_fn():\n",
        "        def eval(server_round, parameters, config):\n",
        "            if server_round > 1:\n",
        "              model = SmoteClient(0)\n",
        "              model.set_parameters(parameters)\n",
        "              accuracy, _, metric = model.evaluate(parameters, {})\n",
        "              return accuracy, metric\n",
        "            return 0.0, {}\n",
        "        return eval\n",
        "\n",
        "\n",
        "    def fit_config(server_round):\n",
        "        return {'rnd': server_round}\n",
        "\n",
        "\n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=0.1,\n",
        "        # fraction_evaluate=0.1,\n",
        "        min_fit_clients=pool_size,\n",
        "        min_evaluate_clients=1,\n",
        "        min_available_clients=pool_size,  # All clients should be available\n",
        "        on_fit_config_fn=fit_config,\n",
        "        evaluate_fn=get_evaluate_fn(),  # centralised evaluation of global mode\n",
        "    )\n",
        "\n",
        "    def client_fn(cid):\n",
        "        return SmoteClient(cid)\n",
        "    \n",
        "    client_resources = {\n",
        "       \"num_cpus\": 1,\n",
        "       \"num_gpus\": 0.0,\n",
        "    } \n",
        "\n",
        "    # start simulation\n",
        "    fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=pool_size,\n",
        "        config=fl.server.ServerConfig(num_rounds=3),\n",
        "        client_resources = client_resources,\n",
        "        strategy=strategy,\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MbiC9HFKA5zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Federated Learning Models - Unbalanced"
      ],
      "metadata": {
        "id": "vzIT00KffwB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_dataset = load_dataset(f\"jyoung2247/sms_spam\", use_auth_token=access_token)\n",
        "df = generate_splits(mod_dataset)\n",
        "experiment = -1\n",
        "fedmlExp('fed_boi_0_0')\n"
      ],
      "metadata": {
        "id": "GQMYXCgBqqDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 0\n",
        "fedmlExp('fed_boi_0_1')"
      ],
      "metadata": {
        "id": "_WjoNhJ5HDtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 1\n",
        "fedmlExp(\"fed_boi_0_2')"
      ],
      "metadata": {
        "id": "thqKz5tzHKbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 2\n",
        "fedmlExp(\"fed_boi_0_3\")"
      ],
      "metadata": {
        "id": "6NORVVn1HPt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Federated Learning Models - Naive Random Oversampling"
      ],
      "metadata": {
        "id": "RNcYqV6Ef0Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_dataset = load_dataset(f\"jyoung2247/sms_spam_random_oversampled\", use_auth_token=access_token)\n",
        "df = generate_splits(mod_dataset)\n",
        "\n",
        "experiment = -1\n",
        "fedmlExp('fed_boi_1_0')"
      ],
      "metadata": {
        "id": "GkCtDwKMqyKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 0\n",
        "fedmlExp('fed_boi_1_1')"
      ],
      "metadata": {
        "id": "BSEruUpTFJva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 1\n",
        "fedmlExp('fed_boi_1_2')"
      ],
      "metadata": {
        "id": "dhjIIjk4FJm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 2\n",
        "fedmlExp('fed_boi_1_3')"
      ],
      "metadata": {
        "id": "7KFLpu6eFMYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Federated Learning Models - Synonym Oversampling"
      ],
      "metadata": {
        "id": "Iet_qrsNf2Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_dataset = load_dataset(f\"jyoung2247/sms_spam_augmented_synonyms\", use_auth_token=access_token)\n",
        "df = generate_splits(mod_dataset)\n",
        "\n",
        "experiment = -1\n",
        "fedmlExp('fed_boi_2_0')"
      ],
      "metadata": {
        "id": "gHXMAbIQq9W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 0\n",
        "fedmlExp('fed_boi_2_1')"
      ],
      "metadata": {
        "id": "-pC8YiEEEeLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 1\n",
        "fedmlExp('fed_boi_2_2')"
      ],
      "metadata": {
        "id": "GCwoTtiREfqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 2\n",
        "fedmlExp('fed_boi_2_3')"
      ],
      "metadata": {
        "id": "JpJPp_3rEgJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Federated Learning Models - Context Substitute Oversampling"
      ],
      "metadata": {
        "id": "EIXjlbv3f34N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_dataset = load_dataset(f\"jyoung2247/sms_spam_augmented_context_substitute\", use_auth_token=access_token)\n",
        "df = generate_splits(mod_dataset)\n",
        "\n",
        "experiment = -1\n",
        "fedmlExp('fed_boi_3_0')"
      ],
      "metadata": {
        "id": "3cNZb13drFD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 0\n",
        "fedmlExp('fed_boi_3_1')"
      ],
      "metadata": {
        "id": "ZKeH3TFGElT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 1\n",
        "fedmlExp('fed_boi_3_2')"
      ],
      "metadata": {
        "id": "M0VIXRvzElRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 2\n",
        "fedmlExp('fed_boi_3_3')"
      ],
      "metadata": {
        "id": "YM5xjV3VElOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Federated Learning Models - Context Insert Oversampling"
      ],
      "metadata": {
        "id": "LOFmF199f5jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_dataset = load_dataset(f\"jyoung2247/sms_spam_augmented_context_insert\", use_auth_token=access_token)\n",
        "df = generate_splits(mod_dataset)\n",
        "\n",
        "experiment = -1\n",
        "fedmlExp('fed_boi_4_0')"
      ],
      "metadata": {
        "id": "dnKz0SgGrSH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 0\n",
        "fedmlExp('fed_boi_4_1')"
      ],
      "metadata": {
        "id": "EAdqbfLIEui0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = -1\n",
        "fedmlExp('fed_boi_4_2')"
      ],
      "metadata": {
        "id": "1JiQBcDyEuSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = -1\n",
        "fedmlExp('fed_boi_4_3')"
      ],
      "metadata": {
        "id": "LbBP6h9XEuPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Federated Learning Models - SMOTE Oversampling"
      ],
      "metadata": {
        "id": "lriqyfeOf8Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oversampledShuffle = oversampled.sample(frac=1)\n",
        "splits = np.array_split(oversampledShuffle, 5)"
      ],
      "metadata": {
        "id": "2st8elchNukb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mod_dataset = load_dataset(f\"jyoung2247/sms_spam_oversampled_SMOTE\", use_auth_token=access_token)\n",
        "mod_dataset = oversampled\n",
        "df = generate_splits(mod_dataset)\n",
        "\n",
        "experiment = -1\n",
        "smoteFL()"
      ],
      "metadata": {
        "id": "OOAdQoA5rr1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 0\n",
        "smoteFL()"
      ],
      "metadata": {
        "id": "MFxgH9t-ctFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 1\n",
        "smoteFL()"
      ],
      "metadata": {
        "id": "cumKY330ctaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 2\n",
        "smoteFL()"
      ],
      "metadata": {
        "id": "p1cglzICctwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Federated Learning Models - Naive Random Undersampling"
      ],
      "metadata": {
        "id": "605wXJqOf9We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_dataset = load_dataset(f\"jyoung2247/sms_spam_undersampled_random\", use_auth_token=access_token)\n",
        "df = generate_splits(mod_dataset)\n",
        "\n",
        "experiment = -1\n",
        "fedmlExp('fed_boi_5_0')"
      ],
      "metadata": {
        "id": "_xqOZYy4rX59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 0\n",
        "fedmlExp('fed_boi_5_1')"
      ],
      "metadata": {
        "id": "F1sO_UPlE1xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 1\n",
        "fedmlExp('fed_boi_5_2')"
      ],
      "metadata": {
        "id": "e1XKSP7rE1mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 2\n",
        "fedmlExp('fed_boi_5_3')"
      ],
      "metadata": {
        "id": "GzycQiYUE1jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Federated Learning Models - CNN Undersampling"
      ],
      "metadata": {
        "id": "x4d4FSHTf_Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_dataset = load_dataset(f\"jyoung2247/sms_spam_undersampled_CNN\", use_auth_token=access_token)\n",
        "df = generate_splits(mod_dataset)\n",
        "\n",
        "experiment = -1\n",
        "fedmlExp('fed_boi_6_0')"
      ],
      "metadata": {
        "id": "EmEveHOJripv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 0\n",
        "fedmlExp('fed_boi_6_1')"
      ],
      "metadata": {
        "id": "vD5M9JyDFAdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 1\n",
        "fedmlExp('fed_boi_6_2')"
      ],
      "metadata": {
        "id": "IgP7B9lSFAZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = 2\n",
        "fedmlExp('fed_boi_6_3')"
      ],
      "metadata": {
        "id": "G61cFIRdFAXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/params"
      ],
      "metadata": {
        "id": "pyCBCVEsuLrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict a single SMS label with Federated model"
      ],
      "metadata": {
        "id": "ALFeojNToBHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/CS6220/fed_boi_0_0\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "SMS_to_predict = \"testSMS\"\n",
        "\n",
        "encoding = tokenizer(SMS_to_predict, padding=\"max_length\", truncation=True, return_tensors='pt', max_length=512)\n",
        "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
        "\n",
        "outputs = trainer.model(**encoding)\n",
        "logits = outputs.logits\n",
        "\n",
        "softmax = torch.nn.Softmax(dim=0)\n",
        "probs = softmax(logits.squeeze())\n",
        "\n",
        "predictionProb, predictionIndex = torch.max(probs, axis=0)\n",
        "\n",
        "print(predictionProb)\n",
        "print(predictionIndex)\n",
        "\n",
        "#get label from prediction\n",
        "labels = dataset[\"train\"].features[\"label\"].names\n",
        "print(labels[predictionIndex.item()])\n",
        "print(predictionProb.item())"
      ],
      "metadata": {
        "id": "BZ_NIHvdoSk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LFp_w5jotztL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "CbG9olVwfJxA",
        "oCVAMp-9DaGt",
        "VypYq7FfDYF0",
        "7CnC2EYBDWRG",
        "1hluJiqLDTSz",
        "WooghEx1DOvs",
        "IREaYEBxDrkg",
        "zMmRylgVKGuA",
        "gml7n9yIfSEJ",
        "Aiwh2L1bF2DV",
        "QV-Qi-P5GH7n",
        "IAo14jBPGQ8g",
        "ZUj1WNkgajOX",
        "5cDJJTqaHQmF",
        "ZtbWFaisHUmy",
        "pBuAlAFLHoP5",
        "QfJ6IZ88L-4J",
        "Y_HBvB-2ftd5",
        "Dxg45aa2NS3c",
        "vzIT00KffwB_",
        "RNcYqV6Ef0Bv",
        "Iet_qrsNf2Ij",
        "EIXjlbv3f34N",
        "LOFmF199f5jp",
        "lriqyfeOf8Aw",
        "605wXJqOf9We",
        "x4d4FSHTf_Em",
        "ALFeojNToBHE"
      ],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}